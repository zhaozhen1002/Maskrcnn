{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pretrained model to C:\\Users\\pasonatech\\Desktop\\car_detection\\tensorflow\\Mask_RCNN\\mask_rcnn_coco.h5 ...\n",
      "... done downloading pretrained model!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0922 10:32:45.087541 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0922 10:32:45.144534 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0922 10:32:45.188582 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0922 10:32:45.227576 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0922 10:32:45.238535 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0922 10:32:48.174577 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0922 10:32:49.656565 18812 deprecation.py:323] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0922 10:32:49.822567 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Desktop\\car_detection\\tensorflow\\Mask_RCNN\\mrcnn\\model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
      "\n",
      "W0922 10:32:49.907577 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Desktop\\car_detection\\tensorflow\\Mask_RCNN\\mrcnn\\utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0922 10:32:49.939574 18812 deprecation.py:506] From C:\\Users\\pasonatech\\Desktop\\car_detection\\tensorflow\\Mask_RCNN\\mrcnn\\model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\pasonatech\\Desktop\\car_detection\\tensorflow\\Mask_RCNN\\logs\\shapes20190922T1033\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 10:33:23.179643 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "W0922 10:33:30.520613 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0922 10:33:30.521638 18812 deprecation_wrapper.py:119] From C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 1382s 14s/step - loss: 1.8056 - rpn_class_loss: 0.0327 - rpn_bbox_loss: 0.6437 - mrcnn_class_loss: 0.3504 - mrcnn_bbox_loss: 0.4270 - mrcnn_mask_loss: 0.3517 - val_loss: 0.9588 - val_rpn_class_loss: 0.0160 - val_rpn_bbox_loss: 0.4547 - val_mrcnn_class_loss: 0.1765 - val_mrcnn_bbox_loss: 0.1689 - val_mrcnn_mask_loss: 0.1426\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.0001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\pasonatech\\Desktop\\car_detection\\tensorflow\\Mask_RCNN\\logs\\shapes20190922T1033\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\pasonatech\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "100/100 [==============================] - 1619s 16s/step - loss: 0.8414 - rpn_class_loss: 0.0168 - rpn_bbox_loss: 0.4279 - mrcnn_class_loss: 0.1280 - mrcnn_bbox_loss: 0.1354 - mrcnn_mask_loss: 0.1332 - val_loss: 0.9084 - val_rpn_class_loss: 0.0165 - val_rpn_bbox_loss: 0.5164 - val_mrcnn_class_loss: 0.1135 - val_mrcnn_bbox_loss: 0.1276 - val_mrcnn_mask_loss: 0.1345\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ShapesConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-53b45c7b3d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mInferenceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mShapesConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mGPU_COUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mIMAGES_PER_GPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minference_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInferenceConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ShapesConfig' is not defined"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1323a9ae3a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test on a random image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moriginal_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_class_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_mask\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     modellib.load_image_gt(dataset_val, inference_config, \n\u001b[1;32m      5\u001b[0m                            image_id, use_mini_mask=False)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-29454f7a88ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n\u001b[1;32m      5\u001b[0m                             dataset_val.class_names, r['scores'], ax=get_ax())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.95\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
